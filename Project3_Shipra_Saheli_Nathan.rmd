---
title: "Project3"
author: "Shipra Ahuja"
date: "July 3, 2016"
output: html_document
---

# Introduction

This project uses data from 1200 restaurants in New York City to demonstrate how singular value decomposition can be implemented in the context of a recommender system. Each restaurant in the data is associated with a number of different feature codes. Each of these codes represent with one of 256 descriptive features detailed in a separate dataset. These features can be the kind of food served (i.e. "Sushi") or another quality of the restaurant (i.e. "good for kids", "open late", etc.).

These datasets have been read into R and a matrix has been created to assign scores between 1 and 10 for each feature code of a restaurant using random number sampling. Therefore each feature of each restaurant has an individual rating.

The created matrix therefore has dimensions of 1200 x 256, and each cell contains either a rating between 1 and 10 if that restaurant has that specific feature, or a 0 otherwise.

Singular Value Decomposition is then performed on this matrix to obtain the Diagonal, Left Singular, and Right Singular matrices. 


# Code in R

```{r,warning=FALSE}

suppressPackageStartupMessages(library(recommenderlab))

# Read the restaurants dataset with the feature codes
restaurant.data<- read.csv("ny_features.csv")

# Read the feature code descriptions
features.data<- read.csv("features.txt", header = FALSE, sep = "\t")[,2]

# Extract restaurant names
restaurant.name=restaurant.data[,1]

# Extract all features codes 
restaurant.data<- restaurant.data[,2:ncol(restaurant.data)]

# Create a blank matrix to hold feature code scores
restaurant.score.matrix<- matrix(1, nrow=nrow(restaurant.data), ncol=257)

# Assign row names to the matrix
rownames(restaurant.score.matrix) = restaurant.name

# Assign colnames to the matrix
colnames(restaurant.score.matrix) = features.data

# Populate the scores for the feature codes using random numbers

for(i in 1:nrow(restaurant.score.matrix)){
  features<- na.omit(as.numeric(restaurant.data[i,]))
  for(j in 1: length(features)){
    restaurant.score.matrix[i, features[j]+1]=sample(2:10, 1)
  }
}

write.table(restaurant.score.matrix, "Restaurant_score.txt", sep = ' ')

# Histogram showing count restaurants with various feature scores

library(ggplot2)

# Read the score file
rest_score <- read.csv("\\cuny\\Restaurant_score.csv",header=TRUE)

# Cleanup columns names
colnames(rest_score)[163:166] <- c("Below $15","$15-$30","$30-$50","Over $50")

# Remove duplicate columns
rest_score <- rest_score[-c(167:172)]

# Get the names of the restaurants
ext_name <- rest_score[,1]

# Get all columns with feature scores
ext_scores <- rest_score[,2:252]

# Get the mean of all rows
Rowmeans <- apply(ext_scores,1,mean)

df <- cbind(ext_name,ext_scores)

```

The histogram below shows that 350+  restaurants in New York city have an average feature score between 0.1-0.15.

There are very few restaurants in New York city which have an average feature score between 0.3-0.5.

Majority of the restaurants have an average feature score between 0.15-0.3.

```{r,warning=FALSE}

# Histogram showing count of restaurants with various feature scores

ggplot(df,aes(Rowmeans)) + geom_histogram(bins=10,fill="orange") + ggtitle("Count of Restaurants with varied Feature Scores") + xlab("Feature score")

# Perform SVD on the matrix
s <- svd(restaurant.score.matrix)

# Get the diagonal matrix
D <- s$d

# Get the left singular matrix
U <- s$u

# Get the right singular matrix
V <- s$v

# Compute the dot product
dotpdt <- U %*% V

# Create the recommendation system

r <- as(dotpdt, "realRatingMatrix")

reco.model <- Recommender(r[1:nrow(r)],method="UBCF",param=list(method="Cosine",k=30))

rest.pred <- predict(reco.model,r[1:nrow(r)],n=5,type="ratings")

```


# Code in Scala

package org.scala.spark

import org.apache.spark.SparkContext

import org.apache.spark.SparkConf

import org.apache.spark.SparkContext._

import org.apache.spark.rdd.RDD

import java.io.PrintWriter

import org.apache.spark.mllib.linalg.Vectors

import org.apache.spark.mllib.linalg.Matrix

import org.apache.spark.mllib.linalg.Vector

import org.apache.spark.mllib.linalg.distributed.RowMatrix

import org.apache.spark.mllib.linalg.SingularValueDecomposition

object svd  {

def main(args: Array[String]): Unit = {

val spConfig = (new SparkConf).setMaster("local").setAppName("SparkSVDDemo")

val sc = new SparkContext(spConfig)

// Load and parse the data file.

val rows = sc.textFile("Restaurant_score.txt").map { line =>

val values = line.split('\t').map(_.toDouble)

Vectors.dense(values)

}


val mat = new RowMatrix(rows)

mat.rows.foreach (println)

// Compute SVD

val svd = mat.computeSVD(mat.numCols().toInt, computeU = true)

val U: RowMatrix = svd.U

val s: Vector = svd.s

val V: Matrix = svd.V

println("Left Singular vectors :")

U.rows.foreach(println)

var rdd = U.rows.map( x => x.toArray.mkString(","))

exportRowMatrix(rdd, "U-matrix.csv")

println("Singular values are :")

println(s)

println("Right Singular vectors :")

println(V)

println("Matrix product :")

val U_mult_V:RowMatrix= U.multiply(V)

rdd = U_mult_V.rows.map( x => x.toArray.mkString(","))

exportRowMatrix(rdd, "Result_matrix.csv")

//U_mult_V.rows.foreach(println)

sc.stop

}

def exportRowMatrix(matrix:RDD[String], fileName: String) = {

    val pw = new PrintWriter(fileName)

    matrix.collect().foreach(line => pw.println(line))

    pw.flush
    pw.close

  }

}



### Output from Scala

![Output from Scala](Scala_output.jpg)

From the above result, it can be said that if someone chooses Afghan restaurant, The Stanhope will be recommended.


